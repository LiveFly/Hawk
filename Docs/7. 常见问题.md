
# 7. 常见问题

Hawk发布了有两三年，收到了很多用户的反馈，按照经验，本QA能解决你大部分的问题。
本页面您可以通过关键字搜索来获取信息。

[TOC]
## 1. 网页采集器

### 1.1 获得的页面与浏览器上不一样？

这非常常见，考虑到速度，Hawk不会动态执行js请求。而浏览器会大量执行js加载图片等，获得的源码可能有所不同，更不用说搜索得到的XPath了。但这样能获得百倍于浏览器的执行速度，因此这个缺点是值得的。

那如何抓取动态页面呢？对于这种情况，参考`动态嗅探`章节。

即使是同样的页面，用Hawk得到的XPATH与浏览器不一致，为什么呢？

一方面，XPath的表示方法有很多种，这和正则类似。可能看起来不一样的XPath指代的都是同一个节点。另一方面，Chrome会执行js代码，而js可能会改变网页的结构，因此XPath也就对应地发生变化了。这有可能会导致从Chrome拷贝出来的XPath在Hawk中不能使用。

Hawk未来不会考虑支持加入执行js代码的功能，因此，**如果搜索XPath，还请以Hawk得到的结果为准。多搜多看，通常就能建立感觉**。

### 1.2 网页中找不到关键字？

分两种情况：
1. 此时本网页不包含该关键字，建议考虑使用【动态嗅探】
2. 关键字太长：文本中包含不可见字符时经常出现。例如`340[制表符]万`，但用户可能会直接输入340万。Hawk搜索是按照严格字符串匹配的，就会匹配不成功，因此输入短一些，如340。
3. 该信息是标签(attribute)：标签指网页中非正文的信息，很多有价值的内容都属于标签：

![image_1at5q2o3p17ac17cekadlke6cp.png-25.6kB][7]

当提取标签时，请勾选网页采集器中的【提取标签】。否则Hawk是不会执行搜索的。

### 1.3 一个网站要设置好多个页面，配置太繁琐！

可在`系统状态视图`中，将网页采集器拖到下面的`复制`图标上，即可复制多个采集器。这样可一定程度上简化操作。

当一组采集器需要同一组请求参数时，可设置`共享源`自动同步，参考`3.2采集器高级用法`的最后一节。


##  2. 数据清洗

### 2.1 拖入`从爬虫转换`后没有任何数据。

`从爬虫转换`实际上搭建了采集器与数据清洗的桥梁，它要选择对应的网页采集器才行，

### 2.2 拖入`从爬虫转换`后，数据有了，但之前的列消失了

参考 `4.3转换器` 中`UDAF的特别说明`.

## 3. 编译与运行问题

虽然在GitHub上是最新的代码，最新代码是可以成功编译的。但不能保证用户是否clone的是早期版本的代码，因此此处罗列可能的编译错误。

### 3.1 编译问题
0. 从GitHub上拉回来的代码，默认启动路径是`Hawk.Core`，这导致编译成功，但运行时提示“无法直接启动带有类库输出类型的项目”： 将Hawk设置为解决方案的启动项目，参考这里：https://jingyan.baidu.com/article/4e5b3e1934c2fc91901e2426.html


1. 因为工程`Hawk.csproj`包含的两个图片文件不存在导致，在这些文件图标上点击右键，选择'排除出项目'，即可正常编译。

2. 找不到项目`System.Windows.???.WPFPropertyGrid.csproj`项目，有两种方法，一种是从作者的GitHub上clone对应的项目，并添加项目引用，另外一种做法是，删除项目引用，添加外部dll引用，所有的外部dll，都能在Include文件夹中找到。

4. 大量的库找不到，几百个报错：

Hawk编译用了不少第三方类库，因此需要配置nuget，它是微软技术栈的pip，能自动安装所需的依赖，配置可参考这里：
> https://docs.nuget.org/


### 3.2 启动后软件只有外边框，没有其他任何显示

老版本的Hawk(<=1.2)在Win7和Win8下的兼容性不佳，请升级最新版本的Hawk.

### 3.3  不小心关掉了某个侧边栏

Hawk采用了Visual Stuido风格的Dock系统，所有的布局都可以调节大小，设定位置，常见的错误是，不小心关闭了某个面板。如何恢复呢？ 对于任务的窗口，双击任务应该就能恢复，如果是日志边栏，不好意思，恢复不了，重启软件吧。

## 4. 宏观问题

和具体使用无关，主要涉及对一些吐槽的回答

### 4.1 为什么只支持Windows？

笔者曾是微软技术栈的粉丝，因此C#,WPF成了设计首选，如今虽然技术栈大大扩展，但用Js或其他语言重写成本太高，因此依然只提供Windows版本。 虽然笔者平时也只用MAC了...

### 4.2 为什么不提供更强的代理？
 
爬虫是一种灰色的应用，虽然作为Hawk的设计者，但我依然不得不这么说。

各大网站都在收集和整理数据上花费了大量的精力，因此抓取的数据应当仅仅作为科学研究使用。作者对Hawk的使用者的行为，不承担任何连带责任。

建议您理性使用爬虫，在不影响网站正常运营的情况下抓取数据。Hawk的好处是，较大地降低了爬虫的开发成本，能让普通用户也能使用。在这一理念下，我们仅仅提供最为实用的功能，而更多高级的功能则不会提供。比如代理切换和验证码识别。

在并行模式下，仅提供单机并行，而分布式并行也不会提供。尽管如此，我们还是会简单讨论如何验证码识别,代理和并行的问题。

代理实现并不复杂，在代码层面上只需要几行代码。但基于之前提过的原因，开源版本不提供代理的支持。


### 4.3 验证码识别？

验证码识别确实有难度，因为各大网站都不相同，简单的如普通四位数字验证码，难的如12306的变态验证码。因此提供通用的识别几乎是不可能的。

如果您愿意付费，并使用第三方的图形验证码服务，则可以将其配置为一个网页采集器，再调用之。

### 4.4 为什么只支持MongoDB和Sqlite?

Hawk在设计之初，就是以弱schema风格定义的。没有严格的列名和列属性。用C#这样的静态强类型语言编写Hawk，其实并不方便。但弱schema让Hawk变得更灵活更强大。

因此，Hawk虽然之前支持各种数据库连接器，而目前只支持MongoDB这样的文档型数据库。之所以不支持传统SQL，是因为获取的数据可能并不满足这些SQL数据库的约束：如列的顺序，列的字段类型，是否为空...很容易导致插入失败。使用Hawk的一般不是程序员，我不想给普通人挖这样的坑。


当然，从各类SQL数据库中读入数据也是可以的，但既然没有提供写入，我们也就索性不提供读入了。需要的话，你可以自己扩展其他数据库连接器。

不过，简单的才是最好的，以作者的经验，使用MongoDB这样的数据库来应对爬虫已经足够了：不需事先建表，高性能，低成本，低维护。我们也不可能一次性就把数据规约成你想要的形式，之后完全可以用其他工具和代码，再将MongoDB的数据导出来，写入到目标数据库。


各种问题还在进一步补充和完善中...


